{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Roughness Parameters in Computer Emulators\"\noutput: html_document\n---\n\n```{r libraries, include = FALSE}\nlibrary(MUCM)\nlibrary(Datasets)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\n```\n\n## Introduction\nThe project objective is to test different methods of parameter estimation within Gaussian process emulators for computer models. Specifically, this project focuses on the \"roughness\" parameter, ${\\phi}$, which summarises how smooth the graph appears and how adjacent inputs interact. \n\n## Emulator Log Likelihood\nUsing the SWAN dataset, a training dataset is specified and an emulator is fitted. Below shows a plot of the loglikelihood when 100 inputs are used as the training dataset:\n```{r organise swan dataset, include = FALSE}\n### Shuffle data\nswan.data <- cbind(TC2.SWAN.inputs, TC2.SWAN.outputs[,1])\nswan.data.shuffled <- swan.data[sample(nrow(swan.data)),]\nTC2.SWAN.inputs <- swan.data.shuffled[ , 1:6]\nTC2.SWAN.outputs[ , 1] <- swan.data.shuffled[ , 7]\n\n###Scale data\nSWAN.col.max <- apply(TC2.SWAN.inputs, 2, max)\nSWAN.col.min <- apply(TC2.SWAN.inputs, 2, min)\nSWAN.col.range <- matrix(SWAN.col.max - SWAN.col.min,\n                         nrow = nrow(TC2.SWAN.inputs),\n                         ncol = ncol(TC2.SWAN.inputs),\n                         byrow = T)\nSWAN.scaled <- (TC2.SWAN.inputs - matrix(SWAN.col.min, nrow = nrow(TC2.SWAN.inputs),\n                                         ncol = ncol(TC2.SWAN.inputs),\n                                         byrow = T) ) / SWAN.col.range\n\n```\n\n```{r Emulator, echo = FALSE}\n###Fit emulator###\nN <- 100\nem3 <- fitEmulator(inputs = SWAN.scaled[1:100,],\n                   outputs = TC2.SWAN.outputs[1:100, 1],\n                   MCMC.iterations= 1000,\n                   MC.plot = TRUE)\n\n```\n\n## Prior Range for Phi \nBefore performing bayesian analysis on the roughness parameters, a prior uniform distribution is fitted. These plots shows what I consider suitable boundaries for the roughness parameter. They are produced by sampling from a normal sample with mean zero and a gaussian correlation function whose ${\\phi}$ values are log(10) and log(0.01) respectively.\n```{r wiggly function, include = FALSE}\nN.range <- 100\nwiggly1 <- function(param, phi_trial){\n  x <- SWAN.scaled[1:N.range, param]\n  x1 <- x[order(x)]\n  A <- corGaussian(inputs = x1, phi = log(phi_trial))\n  y <- GPSample(1, matrix(0, N.range, 1), A)\n  \n  ###GGplot2 data frame###\n  df3 <- data.frame(x.vals = x1, y.vals = y)\n  \n  wiggly.plot <- ggplot(df3, aes(x=x.vals, y = y.vals)) +\n    geom_line() + \n    ylab(\"y\")\n    \n  \n  return(wiggly.plot)\n}\n\n```\n\n```{r wiggly plots, echo = FALSE, , warning=FALSE}\nplot5 <- wiggly1(1, 10) +\n  xlab(\"Wind Speed\") +\n  labs(title = (expression(delta == 10)))\nplot6 <- wiggly1(1, 0.01) +\n  xlab(\"Wind Speed\") + \n  labs(title = expression(delta == 0.01))\ngrid.arrange(plot5, plot6, ncol = 2)\n\n```\n\nSince all the inputs have been scaled to lie between 0 and 1, this above plot is valid for each variable. This produces the following prior distribution for ${\\phi}$:\n\n```{r prior, echo = FALSE}\ndelta.range <- matrix(rep(c(0.01,10), 6),\n                      ncol = 6,\n                      nrow = 2)\n(phi.range <- log(delta.range))\n\n\n\n```\n\n\n## MCMC\nThe plot below shows the MCMC run of the ${\\phi}$ parameter using Metropolis within Gibbs sampling. ${\\phi}$ is constricted to lie inside the prior range, so the acceptance propbability is set to zero if the proposal value of ${\\phi}$ is outside the range.\n\n```{r Gibbs, include = FALSE}\nmetGibbs <- function(inputs, outputs, fn, H, MCMC.iterations, starting.values, \n                     proposal.sd = rep(0.1, ncol(inputs)), cor.function, MC.plot = TRUE, ...) {\n  \n  \n  # H<-make.prior.mean.regressors(inputs) m <- ncol(H)\n  theta.sample <- matrix(0, MCMC.iterations + 1, length(starting.values))\n  n.theta <- ncol(theta.sample)\n  density.sample <- rep(0, MCMC.iterations + 1)\n  theta.sample[1, ] <- starting.values\n  density.sample[1] <- -fn(starting.values, inputs = inputs, H = H, outputs = outputs, cor.function = cor.function, ...)\n  current.density <- density.sample[1]\n  \n  if (MCMC.iterations >= 1) {\n    for (i in 1:MCMC.iterations) {\n      current.theta <- theta.sample[i, ]\n      for (j in 1:n.theta) {\n        proposal <- current.theta\n        proposal[j] <- rnorm(1, current.theta[j], proposal.sd[j])\n        new.density <- -fn(proposal, inputs = inputs, H = H, outputs = outputs, cor.function = cor.function, ...)\n        \n        u <- runif(1)\n        if(phi.range[1,j] < proposal[j] & proposal[j] < phi.range[2,j]){\n          acceptance.prob <- exp(new.density - current.density)\n        }else{\n          acceptance.prob <- 0\n        }\n        if (u < acceptance.prob ) {\n          current.theta <- proposal\n          current.density <- new.density\n        }\n      }\n      theta.sample[i + 1, ] <- current.theta\n      density.sample[i + 1] <- current.density\n    }\n    \n    if (MC.plot == TRUE) \n      try(plot(0:MCMC.iterations, density.sample, type = \"l\", xlab = \"iteration\", ylab = \"log likelihood\"))\n  }\n  list(density.sample = density.sample, theta.sample = theta.sample)\n}\n\n\n```\n\n```{r MCMC phi, echo = FALSE}\nN.MCMC <- 1000\nmcmc <- metGibbs(inputs = em3$training.inputs,\n                            outputs = em3$training.outputs,\n                            fn = negLogLik,\n                            H = em3$H.training, \n                            MCMC.iterations = N.MCMC,\n                            starting.values = c(em3$phi.hat[1:4], 2, em3$phi.hat[6]),\n                            proposal.sd = c(0.5, 0.75, 1.5, 1.25, 1.5, 0.5),\n                            cor.function = corGaussian,\n                            MC.plot = FALSE)\n\n###Plot theta MCMC iterations###\nlog.delta.sample <- mcmc$theta.sample\ndf2 <- data.frame(log.delta.val = as.vector(log.delta.sample),\n                  iteration = rep(1:(N.MCMC+1), 6),\n                  log.delta = as.factor(rep(1:6, each = (N.MCMC+1))))\nlevels(df2$log.delta) <- c(\"Wind Speed\", \"Wave Height\", \"Pak Wave Period\",\n                           \"Water Level\", \"Wind Direction\", \"Wave Direction\")\n\npl1 <- ggplot(data = df2, aes(x=iteration, y=log.delta.val, colour = log.delta)) +\n  geom_point(size = 0.5) +\n  geom_line() +\n  ylab(expression(phi))\npl1\n\n```\n\nThe proposal distribution standard deviation is adjusted until the acceptance probabilities are suitable and autocorrelation plots decay quickly:\n\n```{r, acceptance prob, echo = FALSE}\n###Acceptance probability###\ndf1<-as.data.frame(mcmc$theta.sample)\ndistinct_per_col <- sapply(df1, function(x) length(unique(x)))\nacceptance_prob <- distinct_per_col/N.MCMC\nacceptance_prob \n\n```\n\n```{r acf plots, echo = FALSE}\npar(mfrow = c(3,2))\nacf(mcmc$theta.sample[,1], main = \"Wind Speed\")\nacf(mcmc$theta.sample[,2], main = \"Wave Height\")\nacf(mcmc$theta.sample[,3], main = \"Peak Wave Period\")\nacf(mcmc$theta.sample[,4], main = \"Water Level\")\nacf(mcmc$theta.sample[,5], main = \"Wind Direction\")\nacf(mcmc$theta.sample[,6], main = \"Wave Direction\")\n\n```\n\n## Predictions at New Inputs\n```{r outputs plots, include = FALSE}\n### N = size of trining dataset\n### N.MCMC = iterations\na <- 10\npr<- function(x, N){\n  em3<-  fitEmulator(inputs = SWAN.scaled[1:N,],\n                     outputs = TC2.SWAN.outputs[1:N, 1],\n                     MCMC.iterations = 0,\n                     MC.plot = FALSE,\n                     phi.opt= x)\n  pr1<-predict(em3, SWAN.scaled[(nrow(TC2.SWAN.inputs)-2*a-1):(nrow(TC2.SWAN.inputs)-a-1), ], var.cov = T, sd=F)\n  sample_pr <- GPSample(1, pr1$posterior.mean, pr1$posterior.var)\n  return(sample_pr)\n  }\n\noutput.plot <- function(N.MCMC, N){\n  ###Values directly from emulator###\n  set.seed(N)\n  em1 <- fitEmulator(inputs = SWAN.scaled[1:N,],\n                     outputs = TC2.SWAN.outputs[1:N, 1],\n                     MCMC.iterations= 0,\n                     MC.plot = FALSE)\n  pr4 <- predict (em1, SWAN.scaled[(nrow(SWAN.scaled)-a):nrow(SWAN.scaled), ], var.cov = F, sd=T)\n  \n  set.seed(N+N.MCMC)\n  mcmc <- metGibbs(inputs = em1$training.inputs,\n                   outputs = em1$training.outputs,\n                   fn = negLogLik,\n                   H = em1$H.training, \n                   MCMC.iterations = N.MCMC,\n                   starting.values = em1$phi.hat,\n                   proposal.sd = c(0.5, 0.75, 1.5, 1.25, 1.5, 0.5),\n                   cor.function = corGaussian,\n                   MC.plot = FALSE)\n  log.delta.sample <- mcmc$theta.sample\n  a <- 10\n  pr3 <- t(apply(log.delta.sample, 1, function(x) pr(x, N)))\n  predictions1 <- data.frame(predicted_output = c(colMeans(pr3, dims = 1), pr4$posterior.mean),\n                             true_output = TC2.SWAN.outputs[(nrow(TC2.SWAN.inputs)-2*a-1):nrow(TC2.SWAN.inputs), 1],\n                             key = rep(c(\"emulator\",\"mcmc\"), each = (a+1)))\n  \n  sd <- c((apply(pr3, 2, var))**0.5, pr4$standard.deviation)\n  limits <- aes(ymin = predicted_output - 1.96*sd, ymax = predicted_output + 1.96*sd)\n  \n  pl2 <- ggplot(predictions1, aes(y = predicted_output, x = true_output, colour = key)) +\n    geom_point(size = 0.5) +\n    geom_errorbar(limits) +\n    geom_abline(slope = 1, intercept = 0) +\n    labs(x = \"true output\", y = \"predicted output\") + \n    coord_fixed() +\n    scale_y_continuous(limit = c(0,2)) +\n    scale_x_continuous(limit = c(0,2))\n  return(pl2)\n}\n```\n\nFor 10 new inputs, we are interested in the predicted output generated by the emulator. For each iteration of the MCMC above, run a separate emulator with the corresponding ${\\phi}$ value. This produces 1000 predictions at each of the 10 new inputs. The method of Monte Carlo is then implemented; with these predictions, the mean and standard deviations are found. Below compares these Monte Carlo means with the true output. It also shows the Monte carlo 95% intervals (blue errorbars) and 95% intervals calculated by one run of the emulator (red errorbars) using the calculated standard deviation from the emulator.\n\n```{r plot N.MCMC 1000 N 100, echo = FALSE, warning = FALSE}\noutput.plot(N.MCMC = 100, N = 100) +\n  labs(title = paste(\"MCMC Iterations = 1000 \\nSize of training data = 100\"))\n```\n\n\nIf the emulator was 100% accurate, all the points would lie along the black line with equation x=y. In any dataset there is error, so the true outputs may not fit exactly to a trend. It is clear that this emultor predicts close to the true output and the 95% confidence interval width is almost identical whether we calculate the standard deviation using Monte Carlo or taken directly from the emulator. \n\nThe comparison plot below summarises the effect of increasing the number of MCMC iterations to 5000. The 95% confidence intervals are equal in length between the two plots - this indicates that the Markov chain converges to the stationary distribution quickly i.e running the MCMC for an excessive amount of time is unnecessary as increasing the number of iterations has little effect on emulator prediction accuracy.\n\n```{r increase N.MCMC, echo = FALSE, warning = FALSE}\n\nplot1 <- output.plot(N.MCMC = 1000, N = 100) +\n  labs(title = paste(\"MCMC Iterations = 1000 \\nSize of training data = 100\"))\nplot2 <- output.plot(N.MCMC = 5000, N = 100) +\n    labs(title = paste(\"MCMC Iterations = 5000 \\nSize of training data = 100\"))\n\ngrid.arrange(plot1, plot2, ncol = 2)\n```\n\n\n## Size of Training Dataset\nBelow shows the reults of increasing the size of the training dataset:\n\n```{r increase N, echo = FALSE, warning = FALSE}\n\nplot3 <- output.plot(N.MCMC = 1000, N = 100) +\n  labs(title = paste(\"MCMC Iterations = 1000 \\nSize of training data = 100\"))\nplot4 <- output.plot(N.MCMC = 1000, N = 500) +\n    labs(title = paste(\"MCMC Iterations = 1000 \\nSize of training data = 500\"))\n\ngrid.arrange(plot3, plot4, ncol = 2)\n```\nChanging the size of the training data again has little effect on the accuracy of the emulator. Both methods appear to predict at new inputs with equal error. There is little to distinguish loglikelihod methods and MCMC methods when estimating roughness parameters.",
    "created" : 1474467303794.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3141816024",
    "id" : "F2C6D3D4",
    "lastKnownWriteTime" : 1474467309,
    "last_content_update" : 1474467309847,
    "path" : "~/Documents/Not backed up/Abigail/project.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 14,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}